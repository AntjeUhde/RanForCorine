{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impressions on the lc_classif python package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. Import of Necessary Python Modules\n",
    "2. Conduct resampling and subsetting of data and mask\n",
    "3. Import of Data and First Impressions on Classes\n",
    "4. Analyze and Impute Missing Values\n",
    "5. Analyze Class Separability\n",
    "6. Split into Test and Training Dataset\n",
    "7. Basic Random Forest Model\n",
    "8. Tuning Base Model\n",
    "9. Base Model with Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import of Necessary Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RanForCorine import geodata_handling as datahandler\n",
    "from RanForCorine import data_cleaning as clean\n",
    "from RanForCorine import descriptive_stats as descr\n",
    "from RanForCorine import separability as sep\n",
    "from RanForCorine import test_training_separation as tts\n",
    "from RanForCorine import randomforest_classifier as rf\n",
    "from RanForCorine import accuracy as acc\n",
    "from RanForCorine import tuning as tuning\n",
    "from RanForCorine import feature_importance as feat\n",
    "from RanForCorine import visualization as vis\n",
    "import numpy as np\n",
    "import osr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Resampling and subsetting of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the selection of the classes and training the random forest model, a preparation of the data might be necessary. The data and land cover mask should have the same extend and resolution. The Sentinel-1 data will then be split into the backscatter values of the classes in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the nexessary file paths\n",
    "fp_stack=r\"Path\\to\\Sentinel1\\data\"\n",
    "fp_hdr=r\"Path\\to\\Sentinel1\\data.hdr\"\n",
    "fp_mask=r\"Path\\to\\CorineLandCover\\mask.tif\"\n",
    "fp_s1_res=r'Path\\to\\save\\resampled\\Sentinel1'\n",
    "fp_mask_sub=r'Path\\to\\save\\subsetted\\CLC\\mask.tif'\n",
    "fp_csv=r'Path\\to\\save\\splitted\\data.csv'\n",
    "fp_out=r'Path\\to\\save\\result.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adjust pixel size and extend of S-1 data and mask\n",
    "datahandler.adjust(fp_stack,fp_mask, epsg=32633, write=True, outfp1=fp_s1_res,\\\n",
    "                    outfp2=fp_mask_sub,hdrfp=fp_hdr,subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the Sentinel-1 data into the Corine land Cover classes and return the DataFrame\n",
    "data_raw=datahandler.split_classes(fp_s1_res,fp_mask_sub)\n",
    "data_raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. First Impressions on Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section data is imported and first impressions on the dataset are shown. For example there is a barplot showing how many pixels/values there are per class. As it will be seen later on, the amount of pixels has a substantial impact on the class separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPORT DATA ################################\n",
    "# data_raw = datahandler.importCSV(r\"Path\\to\\splitted\\data.csv\").drop(data_raw.columns[0], axis=1) #remove index column\n",
    "print(data_raw.columns)\n",
    "\n",
    "# FIRST IMPRESSIONS ##########################\n",
    "class_count = descr.countPxlsPerClass(data_raw, \"Label_nr\")\n",
    "class_count.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore it is possible to plot histograms for selected classes. As shown below for the data of Vattenrike class distributions are very differnt throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "descr.plotHist(data_raw, 211, \"Label_nr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "descr.plotHist(data_raw, 522, \"Label_nr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analyze and Impute Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Random Forest model used in this package is based on scikit-learn it cannot operate on null values in the data. Before computing the model it is neccessary to get rid of any missing values. The package provides the opportunity to count missing data fields and impute them with the mean of the column where it is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISSING VALUES #############################\n",
    "print(\"--- ANALYZING MISSING VALUES ---\")\n",
    "missing_count = clean.countMissingValuesTotal(data_raw, null_value=-99.0)\n",
    "print(str(missing_count) + \" values are missing.\")\n",
    "# Impute missing values with mean in column\n",
    "data_imp = clean.imputeMean(data_raw, clean=True, null_value=-99.0)\n",
    "print(\"Done imputing missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analyze Class Separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are so many classes in the dataset, it is recommended to analyze their separability. For this purpose three different distance measures can be calculated. In this example the euclidean distance is shown in a heatmap clearly visualizing the best separable classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS SEPARABILITY ##########################\n",
    "print(\"--- ANALYZING CLASS SEPARABILITY ---\")\n",
    "class_sep = sep.calcSep(data_imp, \"Label_nr\")\n",
    "sep.printHeatmap(class_sep, \"Euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use only those classes for the model it is neccessary to compress all remaining classes to one class. The plot shows that now there are approximately 15,000 pixels of the class 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we decided to use following classes\n",
    "sep_list = [112,211,231,311,312,523,512]\n",
    "\n",
    "# add column to dataset setting all unwanted class values to 0\n",
    "data_comp = clean.compressClasses(data_imp, sep_list, \"Label_nr\", \"Label_new\")\n",
    "# count number of pixels per class\n",
    "class_count_comp = descr.countPxlsPerClass(data_comp, \"Label_new\")\n",
    "class_count_comp.plot.bar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "descr.plotHist(data_comp, 0, \"Label_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new separability analysis compares the remaining classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show new separability\n",
    "class_sep2 = sep.calcSep(data_comp, \"Label_new\")\n",
    "sep.printHeatmap(class_sep2, \"Euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Split into Test and Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the images are split into test and training dataset. Whereas the training dataset needs to be larger and is used to train the model, the test dataset will be used to predict class values. Class labels are split into test and training datasets as well. \n",
    "As this package should be used with image classification, the test dataset is always the upper proportion of the image. In this example we used a proportion of 0.3 for the testsize. So the upper 30 % of the images are used for testing or rather prediction. The lower 70 % are used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST TRAINING #################################\n",
    "print(\"--- Test Training Split ---\")\n",
    "x_train, x_test, y_train, y_test = tts.imageSplit(data_comp, labelcol=\"Label_new\", imagedim=[455,423], testsize=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Basic Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for the basic Random Forest (RF) model. To keep computing time possibly low we decided to go with low max_depth and n_estimator values which definitely results into a model that is not a powerful as it could be.\n",
    "The model is fitted to the training data and after that the prediction is carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RANDOM FOREST BASE MODEL ######################\n",
    "print(\"--- CALCULATING BASE MODEL ---\")\n",
    "base_model = rf.RandomForestClassifier(max_depth=2, random_state=1, n_estimators=50)\n",
    "rf.fitModel(base_model, x_train, y_train)\n",
    "print(\"--- DONE FITTING MODEL ---\")\n",
    "base_pred = rf.predictModel(base_model, x_test)\n",
    "print(\"--- DONE PREDICTING ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score shows that this model only has an accuracy of about 55 %. The confusion matrix is printed to show the predicted versus the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY\n",
    "base_acc = acc.getAccuracy(base_pred, y_test)\n",
    "print(\"Base model has accuracy of: \" + str(base_acc)) \n",
    "base_conf = acc.getConfMatrix(base_pred, y_test)\n",
    "classes = [0, 112, 211, 231, 311, 312, 512, 523]\n",
    "acc.printConfMatrix(base_conf, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted image can be printed clearly showing that only a few classes could be classified with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis.plotResult(base_pred, [127, 455])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Tuning Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is impossible to find the right combination of parameters for a complex RF model it is recommended to use hyperparameter tuning. The best setting is tried to be found using a three-fold cross-validation over a grid of possible parameter values. This clearly shows an improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BASE MODEL TUNING\n",
    "print(\"--- TUNING BASE MODEL ---\")\n",
    "base_params = tuning.getParamsOfModel(base_model)\n",
    "print(base_params)\n",
    "max_depth_base = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "max_depth_base.append(None)\n",
    "base_grid = {'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 200, num = 5)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': max_depth_base}\n",
    "best_base_model = tuning.tuneModel(base_grid, x_train, y_train, n_jobs=1)\n",
    "best_base_pred = rf.predictModel(best_base_model, x_test)\n",
    "best_base_acc = acc.getAccuracy(best_base_pred, y_test)\n",
    "print(\"Tuned base model has accuracy of: \" + str(best_base_acc)) #0.9001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_base_acc = acc.getAccuracy(best_base_pred, y_test)\n",
    "print(\"Tuned base model has accuracy of: \" + str(best_base_acc)) \n",
    "best_base_conf = acc.getConfMatrix(best_base_pred, y_test)\n",
    "classes = [0, 112, 211, 231, 311, 312, 512] #list here the class numbers in ascending order\n",
    "acc.printConfMatrix(best_base_conf, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen in the plotted result all classes could be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis.plotResult(best_base_pred, [127, 455])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Base Model with Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to tuning it is possible to use feature selection. As we got more than 200 features or rather images in this dataset the plot shows the 10 most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.importancePlot(base_model, x_train.columns, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on those important features it is possible to carry out a new classification by training a new model. This slightly improves the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select important features\n",
    "x_important_test, x_important_train = feat.selectImportantFeatures(base_model, x_train, y_train, x_test)\n",
    "sel_model = rf.RandomForestClassifier(max_depth=2, random_state=1, n_estimators=50)\n",
    "rf.fitModel(sel_model, x_important_train, y_train)\n",
    "sel_pred = rf.predictModel(sel_model, x_important_test)\n",
    "sel_acc = acc.getAccuracy(sel_pred, y_test)\n",
    "print(\"Model with selected features as accuracy of: \" + str(sel_acc)) #0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plotResult(sel_pred, [127, 455])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to disk as GeoTIFF\"\n",
    "result=sel_pred.reshape((127, 455))\n",
    "ds=datahandler.read_file_gdal(fp_s1_res)\n",
    "gt=ds.GetGeoTransform()\n",
    "srs=osr.SpatialReference()\n",
    "srs.ImportFromWkt(ds.GetProjection())\n",
    "datahandler.create_gtiff(sel_pred,gt,srs,fp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
